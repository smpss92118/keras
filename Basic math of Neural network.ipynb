{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numpy 只能支援相同形狀相加，所以要把較小的tensor進行broadcasting.\n",
    "    - y要能broadcasting要後面項相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 32, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.random((64,3,32,10))\n",
    "y = np.random.random((32,10))\n",
    "z = np.maximum(x,y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### numpy的dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "<img src = \"https://www.mathsisfun.com/algebra/images/matrix-multiply-a.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2],[1,3]])\n",
    "y = np.array([[1,1],[2,2]])\n",
    "\n",
    "z = np.dot(x,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot的數學表示，用python呈現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_dot(x,y):\n",
    "    z=0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z+=x[i]*y[i] #x[0]y[0]+x[1]y[1].....\n",
    "    return z\n",
    "\n",
    "def matrix_dot(x,y):\n",
    "    z = np.zeros((x.shape[0],y.shape[0])) #建立數值為0的np.array\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i,:]\n",
    "            column_y = y[:,j]\n",
    "            z[i,j] = vector_dot(row_x,column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 5.]\n",
      " [7. 7.]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix_dot(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Type of Neural Network\n",
    "不論哪種神經網路，都是在找層與層之間的權重，不斷變更權重，去最小化損失函數的值。\n",
    "###  gradient descent，GD\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/892/1*zgcke0uQx87GAhCwTsIGag.png\" width=\"40%\">\n",
    "這邊的t是第幾次更新參數，γ是學習率(Learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隨機梯度下降法(Stochastic gradient descent, SGD)\n",
    "\n",
    "   - GD我們是一次用全部訓練集的數據去計算損失函數的梯度就更新一次參數。\n",
    "   - SGD就是一次跑一個樣本或是小批次(mini-batch)樣本然後算出一次梯度或是小批次梯度的平均後就更新一次，\n",
    "        那這個樣本或是小批次的樣本是隨機抽取的，所以才會稱為隨機梯度下降法。\n",
    "        \n",
    "* 而SGD和GD的缺點在於學習率是固定的，如果學習率太小，學習速度太慢，\n",
    "    但學習率太大，雖然還是可以往最佳解走，可是在最後幾部，會走不進最佳解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1400/1*ZoRXwJ3xXxwxXPtKPAsRHg.png\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 這邊的t是第幾次更新參數，γ是學習率(Learning rate)，m是momentum項(一般設定為0.9)\n",
    "- 相對於SGD的優點在於，如果學習率很小，SGD會在local minimum就停住，但是Momentum會跳出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/max/1006/1*db1qSL5OkwvF1uUm_rWexg.png\" width='40%' align = 'left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ε是平滑項，主要避免分母為0的問題，一般設定為1e-7。Gt這邊定義是一個對角矩陣，對角線每一個元素是相對應每一個參數梯度的平方和\n",
    "* Adagrad缺點是在訓練中後段時，有可能因為分母累積越來越大(因為是從第1次梯度到第t次梯度的和)導致γ被分母稀釋變接近於零"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "<img src = \"https://miro.medium.com/max/1290/1*kw_KP5SmJljmVfWTq-mD9g.png\" width='40%' align = 'left'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* E[]在統計上就是取期望值，所以是取gi²的期望值，白話說就是他的平均數。ρ是過去t-1時間的梯度平均數的權重，一般建議設成0.9\n",
    "* 所以就不會出現分母越來越大的問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras的hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=train_images.reshape((60000,28*28))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((10000,28*28))\n",
    "test_images = test_images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2629 - accuracy: 0.9231\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1051 - accuracy: 0.9685\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0686 - accuracy: 0.9793\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0498 - accuracy: 0.9852\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0371 - accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x132e7c668>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512,activation='relu', input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "\n",
    "network.compile(optimizer='rmsprop',loss = 'categorical_crossentropy',metrics =['accuracy'])\n",
    "network.fit(train_images,train_labels,epochs=5,batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
